{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold,StratifiedKFold,ShuffleSplit,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error,r2_score\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from scipy.stats import skew,randint\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importances(model,X):\n",
    "    important_features = pd.Series(data=rf_model.feature_importances_,index=X.columns)\n",
    "    important_features.sort_values(ascending=False,inplace=True)\n",
    "    print(important_features.head(50))\n",
    "    \n",
    "def get_cat_columns_by_type(df):\n",
    "    out = []\n",
    "    for colname,col_values in df.items():\n",
    "        if is_string_dtype(col_values):\n",
    "            out.append((colname,'string') )\n",
    "        elif not is_numeric_dtype(col_values):\n",
    "            out.append((colname,'categorical') )\n",
    "    return out       \n",
    "\n",
    "def get_numeric_columns(df):\n",
    "    out = []\n",
    "    for colname,col_values in df.items():\n",
    "        if is_numeric_dtype(col_values):\n",
    "            out.append(colname)\n",
    "    return out       \n",
    "    \n",
    "def get_missing_values_percentage(df):\n",
    "    missing_values_counts_list = df.isnull().sum()\n",
    "    total_values = np.product(df.shape)\n",
    "    total_missing = missing_values_counts_list.sum()\n",
    "    # percent of data that is missing\n",
    "    return (total_missing/total_values) * 100\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_str_type(df_in,columns,inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    "        \n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def handle_missing_values(df_in,cat_cols=[], num_cols=[],na_dict=None,add_nan_col=True,inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    " \n",
    "    if na_dict is None:\n",
    "        na_dict = {}\n",
    "\n",
    "    for colname, col_values in df.items():   \n",
    "        if colname not in num_cols:\n",
    "            continue\n",
    "        if pd.isnull(col_values).sum():\n",
    "            df[colname+'_na'] = pd.isnull(col_values)\n",
    "            filler = na_dict[colname] if colname in na_dict else col_values.median()\n",
    "            df[colname] = col_values.fillna(filler)\n",
    "            na_dict[colname] = filler\n",
    "    for colname in cat_cols:\n",
    "        if colname not in df.columns:\n",
    "            continue\n",
    "        df[colname].fillna(df[colname].mode()[0], inplace=True)\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(df[colname].values)) \n",
    "        df[colname] = lbl.transform(list(df[colname].values))\n",
    "    \n",
    "    return (df,na_dict)\n",
    "\n",
    "\n",
    "\n",
    "def scale_num_cols(df_in, mapper, inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    "        \n",
    "    if mapper is None:\n",
    "        map_f = [([c],StandardScaler()) for c in df.columns if is_numeric_dtype(df[c])]\n",
    "        mapper = DataFrameMapper(map_f).fit(df)\n",
    "    df[mapper.transformed_names_] = mapper.transform(df)\n",
    "    return (df,mapper)\n",
    "\n",
    "\n",
    "\n",
    "def extract_and_drop_target_column(df_in, y_name, inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    "    if not is_numeric_dtype(df[y_name]):\n",
    "        df[y_name] = df[y_name].cat.codes\n",
    "        y = df[y_name].values\n",
    "    else:\n",
    "        y = df[y_name]\n",
    "    df.drop([y_name], axis=1, inplace=True)\n",
    "    return (df,y)\n",
    "\n",
    "def print_mse(m,X_train, X_valid, y_train, y_valid):\n",
    "    res = [mean_squared_error(y_train,m.predict(X_train)),\n",
    "                mean_squared_error(y_valid,m.predict(X_valid)),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    print('MSE Training set = {}, MSE Validation set = {}, score Training Set = {}, score on Validation Set = {}'.format(res[0],res[1],res[2], res[3]))\n",
    "    if hasattr(m, 'oob_score_'):\n",
    "          print('OOB Score = {}'.format(m.oob_score_))      \n",
    "\n",
    "def get_iqr_min_max(df,cols):\n",
    "    out = {}\n",
    "    for colname, col_values in df.items():\n",
    "        if colname not in cols:\n",
    "            continue\n",
    "        quartile75, quartile25 = np.percentile(col_values, [75 ,25])\n",
    "        ## Inter Quartile Range ##\n",
    "        IQR = quartile75 - quartile25\n",
    "        min_value = quartile25 - (IQR*1.5)\n",
    "        max_value = quartile75 + (IQR*1.5)\n",
    "        out[colname] = (min_value,max_value)\n",
    "    return out\n",
    "\n",
    "\n",
    "def bin_numerical_columns(df_in,cols,inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    "        \n",
    "    for col in cols.keys():\n",
    "        bins = cols[col]\n",
    "        buckets_ = np.linspace(bins[0],bins[1],bins[2])\n",
    "        df[col] = pd.cut(df[col],buckets_,include_lowest=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_train,df_test=None,\n",
    "                  log_y=True,\n",
    "                  id_col= None,\n",
    "                  drop_target=True,\n",
    "                  convert_to_cat_cols=None,\n",
    "                  remove_skewness=False,scale_mapper=None,\n",
    "                  bin_columns_dict=None,\n",
    "                  new_features_func=None):\n",
    "    \n",
    "    if drop_target:\n",
    "        df,y = extract_and_drop_target_column(df_train,'SalePrice',inplace=True)\n",
    "    if log_y:\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        y = None\n",
    "        \n",
    "    combined = pd.concat((df, df_test)).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    if id_col is not None:\n",
    "        combined.drop(id_col, axis=1,inplace=True)\n",
    "        if df_test is not None:\n",
    "            test_id = df_test[id_col].copy()\n",
    "        else: test_id = None\n",
    "   \n",
    "    if new_features_func is not None:\n",
    "        combined = new_features_func(combined)\n",
    "    \n",
    "    \n",
    "    if convert_to_cat_cols is not None:\n",
    "        combined = convert_to_str_type(combined,convert_to_cat_cols,inplace=True)\n",
    "    \n",
    "        \n",
    "    if bin_columns_dict is not None:\n",
    "        combined = bin_numerical_columns(combined,bin_columns_dict,inplace=True)\n",
    "    \n",
    "    \n",
    "    cat_cols = get_cat_columns_by_type(combined)\n",
    "    cat_cols = [cat_cols[i][0] for i in range(len(cat_cols))]\n",
    "    num_cols = [col for col in combined.columns if col not in cat_cols]\n",
    "    \n",
    "    combined = pd.get_dummies(combined,columns=cat_cols, dummy_na=True)\n",
    "    \n",
    "    n_train = df.shape[0]\n",
    "    n_test = df_test.shape[0]\n",
    "      \n",
    "    \n",
    "    combined,d = handle_missing_values(combined,cat_cols=cat_cols,\n",
    "                                       num_cols=num_cols,inplace=True)\n",
    "    \n",
    "    \n",
    "    if remove_skewness:\n",
    "        skewed_cols = combined[num_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "        skewness = pd.DataFrame({'Skew' :skewed_cols})\n",
    "        skewness_log = skewness[skewness > 4.0]\n",
    "        skewness_other = skewness[skewness <= 4.0]\n",
    "        skewed_features_log = skewness_log.index\n",
    "        skewed_features_other = skewness_other.index\n",
    "        lambda_ = 0.0\n",
    "        for feature in skewed_features_log:\n",
    "            combined[feature] = boxcox1p(combined[feature],lambda_)\n",
    "        lambda_ = 0.15\n",
    "        for feature in skewed_features_other:\n",
    "            combined[feature] = boxcox1p(combined[feature],lambda_)\n",
    "    \n",
    "    if scale_mapper is not None:\n",
    "        map_f = [([c],scale_mapper) for c in num_cols]\n",
    "        mapper = DataFrameMapper(map_f).fit(combined)\n",
    "    else:\n",
    "        mapper = None\n",
    "        \n",
    "    combined,_ = scale_num_cols(combined,mapper,inplace=True) \n",
    "    \n",
    "    print(get_missing_values_percentage(combined))\n",
    "    \n",
    "    return combined,df,y,cat_cols,num_cols,test_id,n_train,n_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features1(df):\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    return df\n",
    "def add_new_features2(df):\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df[\"OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\n",
    "    return df\n",
    "def add_new_features3(df):\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df[\"OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\n",
    "    df['TotalLivArea'] = df['GrLivArea'] + df['GarageArea'] + df['LotArea']\n",
    "    return df\n",
    "\n",
    "def add_new_features4(df):\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df[\"OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\n",
    "    df['TotalLivArea'] = df['GrLivArea'] + df['GarageArea'] + df['LotArea']\n",
    "    \n",
    "    df[\"GrLivArea-2\"] = df[\"GrLivArea\"] ** 2\n",
    "    df[\"GrLivArea-3\"] = df[\"GrLivArea\"] ** 3\n",
    "    df[\"GrLivArea-Sq\"] = np.sqrt(df[\"GrLivArea\"])\n",
    "    df[\"GarageArea-2\"] = df[\"GarageArea\"] ** 2\n",
    "    df[\"GarageArea-3\"] = df[\"GarageArea\"] ** 3\n",
    "    df[\"GarageArea-Sq\"] = np.sqrt(df[\"GarageArea\"])\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = \"data/iowa_housing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(f'{PATH}train.csv', low_memory=False)\n",
    "df_test = pd.read_csv(f'{PATH}test.csv', low_memory=False)\n",
    "df = df_raw.copy()\n",
    "stratify_col = df['OverallQual'].copy()\n",
    "combined,df,y,cat_cols,num_cols,test_id,n_train,n_test = preprocess_df(\n",
    "                                       df_train=df,df_test=df_test,\n",
    "                                       drop_target=True,\n",
    "                                       new_features_func=add_new_features4,\n",
    "                                       id_col='Id',\n",
    "                                       log_y=True,\n",
    "                                       convert_to_cat_cols=['GarageCars','CentralAir','MoSold','YrSold','YearBuilt','GarageYrBlt','YearRemodAdd'],\n",
    "                                       remove_skewness=True,\n",
    "                                       scale_mapper=RobustScaler(),\n",
    "                                       bin_columns_dict={'OverallQual':(1,11,10),'OverallCond':(1,11,10)} \n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1314, 679), (146, 679), (1314,), (146,), (1314,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = combined[:n_train]\n",
    "df_test = combined[n_train:]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df,y,test_size=0.10,\n",
    "                                  stratify=stratify_col,shuffle = True,random_state=20)\n",
    "\n",
    "stratify_X_train = stratify_col[:X_train.shape[0]].copy()\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape, stratify_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1182, 679), (132, 679), (1182,), (132,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size=0.10,\n",
    "                                  stratify=stratify_X_train,shuffle = True,random_state=20)\n",
    "X_train.shape,X_valid.shape,y_train.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Training set = 0.002790407411352234, MSE Validation set = 0.014356924972681406, score Training Set = 0.9825650364835996, score on Validation Set = 0.8985692390955484\n",
      "OOB Score = 0.8698052744425425\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=1500,n_jobs=-1,oob_score=True).fit(X_train.values,\n",
    "                                                                                y_train)\n",
    "print_mse(rf_model, X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Training set = 0.0001155566817433916, MSE Validation set = 0.014306574057492676, score Training Set = 0.9992779812288072, score on Validation Set = 0.898924965105787\n"
     ]
    }
   ],
   "source": [
    "gb_model = GradientBoostingRegressor(n_estimators=1500,random_state=10).fit(X_train,y_train)\n",
    "print_mse(gb_model, X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Training set = 0.021835061589687035, MSE Validation set = 0.013969604890325395, score Training Set = 0.8635706382352016, score on Validation Set = 0.9013056308188245\n"
     ]
    }
   ],
   "source": [
    "elasticnet_model = ElasticNet(alpha=0.01,l1_ratio=.9,random_state=100).fit(X_train,y_train)\n",
    "print_mse(elasticnet_model, X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
       "             random_state=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avinash.tiwari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':[300,500,800,1100,1500,1800],\n",
    "              'max_features': [0.5,0.7,0.9,'auto'],\n",
    "              'min_samples_split': [2,3,10],\n",
    "              'min_samples_leaf': [1,3,10]}\n",
    "\n",
    "start = time()\n",
    "gridSearch_rf = GridSearchCV(RandomForestRegressor(warm_start=True,n_jobs=8),param_grid=params,n_jobs=8)        \n",
    "gridSearch_rf.fit(X_train,y_train)\n",
    "print('training took {} minutes'.format((time() - start)/60.))\n",
    "\n",
    "print_mse(gridSearch_rf, X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(gridSearch_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gridSearch_rf,'gridSearch_rf_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_rf = joblib.load('gridSearch_rf_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':[300,500,800,1100,1500,1800],\n",
    "              \"max_features\": randint(80,680),\n",
    "              \"min_samples_split\": randint(2, 11),\n",
    "              \"min_samples_leaf\": randint(1, 11),\n",
    "              \"subsample\":[0.6,0.7,0.75,0.8,0.9]\n",
    "         }\n",
    "\n",
    "randomSearch_gb = RandomizedSearchCV(GradientBoostingRegressor(warm_start=True),\n",
    "                                     param_distributions=params,n_iter=20,\n",
    "                                     cv=kfold,n_jobs=6)        \n",
    "randomSearch_gb.fit(X_train,y_train)\n",
    "print_mse(randomSearch_gb, X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(randomSearch_gb,'randomSearch_gb_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSearch_gb = joblib.load('randomSearch_gb_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(randomSearch_gb.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':[300,500,800,1100,1500,1800],\n",
    "              \"max_features\": randint(80,680),\n",
    "              \"min_samples_split\": randint(2, 11),\n",
    "              \"min_samples_leaf\": randint(1, 11)\n",
    "         }\n",
    "\n",
    "randomSearch_rf = RandomizedSearchCV(RandomForestRegressor(warm_start=True),\n",
    "                                     param_distributions=params,cv=kfold,\n",
    "                                     n_jobs=6, n_iter=20)        \n",
    "randomSearch_rf.fit(X_train,y_train)\n",
    "print_mse(randomSearch_rf, X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(randomSearch_rf,'randomSearch_rf_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSearch_rf = joblib.load('randomSearch_rf_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(randomSearch_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha':[0.001,0.01,0.1,1.],\n",
    "          'l1_ratio': [0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "          'max_iter':[1000,2000,5000,10000],\n",
    "          'selection':['cyclic','random']\n",
    "         }\n",
    "\n",
    "randomSearch_elastic = RandomizedSearchCV(ElasticNet(warm_start=True),param_distributions=params,\n",
    "                                          cv=kfold,n_jobs=6, n_iter=20)        \n",
    "randomSearch_elastic.fit(X_train,y_train)\n",
    "print_mse(randomSearch_elastic, X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(randomSearch_elastic,'randomSearch_elastic_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSearch_elastic = joblib.load('randomSearch_elastic_iowa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(randomSearch_elastic.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSearch_elastic1 = ElasticNet(alpha=0.001,\n",
    "                                   selection='cyclic',\n",
    "                                   max_iter=5000,\n",
    "                                   l1_ratio=0.8\n",
    "                                   ).fit(X_train,y_train)\n",
    "print_mse(randomSearch_elastic1,X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSearch_elastic2 = ElasticNet(alpha=0.001,\n",
    "                                   selection='cyclic',\n",
    "                                   max_iter=1000,\n",
    "                                   l1_ratio=0.9\n",
    "                                   ).fit(X_train,y_train)\n",
    "print_mse(randomSearch_elastic2,X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSearch_elastic3 = ElasticNet(alpha=0.001,\n",
    "                                   selection='random',\n",
    "                                   max_iter=5000,\n",
    "                                   l1_ratio=0.9\n",
    "                                   ).fit(X_train,y_train)\n",
    "print_mse(randomSearch_elastic3,X_train,X_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mse(randomSearch_elastic1,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_model = RFE(randomSearch_elastic1).fit(X_train,y_train)\n",
    "print_mse(rfe_model, X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GridSearch RF')\n",
    "print_mse(gridSearch_rf, X_train,X_test,y_train,y_test)\n",
    "print('RandomSearch RF:')\n",
    "print_mse(randomSearch_rf, X_train,X_test,y_train,y_test)\n",
    "print('RandomSearch GB:')\n",
    "print_mse(randomSearch_gb, X_train,X_test,y_train,y_test)\n",
    "print('RandomSearch Elastic:')\n",
    "print_mse(randomSearch_elastic, X_train,X_test,y_train,y_test)\n",
    "print('RFE Elastic:')\n",
    "print_mse(rfe_model, X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
