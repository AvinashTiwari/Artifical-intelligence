{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold,StratifiedKFold,ShuffleSplit,StratifiedShuffleSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures,LabelEncoder,Imputer,RobustScaler, StandardScaler, MinMaxScaler,FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score, make_scorer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "from scipy import stats\n",
    "from scipy.stats import skew,randint\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import randint as sp_randint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importances(model,X):\n",
    "    important_features = pd.Series(data=rf_model.feature_importances_,index=X.columns)\n",
    "    important_features.sort_values(ascending=False,inplace=True)\n",
    "    print(important_features.head(50))\n",
    "    \n",
    "def get_cat_columns_by_type(df):\n",
    "    out = []\n",
    "    for colname,col_values in df.items():\n",
    "        if is_string_dtype(col_values):\n",
    "            out.append((colname,'string') )\n",
    "        elif not is_numeric_dtype(col_values):\n",
    "            out.append((colname,'categorical') )\n",
    "    return out       \n",
    "\n",
    "def get_numeric_columns(df):\n",
    "    out = []\n",
    "    for colname,col_values in df.items():\n",
    "        if is_numeric_dtype(col_values):\n",
    "            out.append(colname)\n",
    "    return out       \n",
    "    \n",
    "def get_missing_values_percentage(df):\n",
    "    missing_values_counts_list = df.isnull().sum()\n",
    "    total_values = np.product(df.shape)\n",
    "    total_missing = missing_values_counts_list.sum()\n",
    "    # percent of data that is missing\n",
    "    return (total_missing/total_values) * 100\n",
    "\n",
    "\n",
    "def convert_to_str_type(df_in,columns,inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    "        \n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "def extract_and_drop_target_column(df_in, y_name, inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    "    if not is_numeric_dtype(df[y_name]):\n",
    "        df[y_name] = df[y_name].cat.codes\n",
    "        y = df[y_name].values\n",
    "    else:\n",
    "        y = df[y_name].copy()\n",
    "    df.drop([y_name], axis=1, inplace=True)\n",
    "    return (df,y)\n",
    "\n",
    "def get_cat_and_numerical_cols(df):\n",
    "    cat_cols = get_cat_columns_by_type(df)\n",
    "    cat_cols = [cat_cols[i][0] for i in range(len(cat_cols))]\n",
    "    num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "    return cat_cols,num_cols\n",
    "\n",
    "    \n",
    "def clean_df(df,pipelines):\n",
    "    cat_cols, num_cols = get_cat_and_numerical_cols(df)\n",
    "    dfs = []\n",
    "    if len(num_cols):\n",
    "        df1 = df[num_cols]\n",
    "        print(df1.shape)\n",
    "        pipelines['pipe_df_missing_num'].fit(df1)\n",
    "        data1 = pipelines['pipe_df_missing_num'].transform(df1)\n",
    "        df1 = pd.DataFrame(data1,columns=num_cols)\n",
    "        dfs.append(df1)\n",
    "    \n",
    "    if len(cat_cols):\n",
    "        df2 = df[cat_cols]\n",
    "        print(df2.shape)\n",
    "        pipelines['pipe_df_missing_cat'].fit(df2)\n",
    "        data2 = pipelines['pipe_df_missing_cat'].transform(df2)     \n",
    "        df2 = pd.DataFrame(data2,columns=cat_cols)\n",
    "        dfs.append(df2)\n",
    "        \n",
    "        \n",
    "    return pd.concat(dfs,axis=1)\n",
    "\n",
    "\n",
    "def handle_encoding(df,one_hot=False):\n",
    "    lbl = LabelEncoder()\n",
    "    cat_cols,_ = get_cat_and_numerical_cols(df)\n",
    "    print('len of cat cols = {}'.format(len(cat_cols)))\n",
    "    for colname in cat_cols:\n",
    "        lbl.fit(list(df[colname].values)) \n",
    "        df[colname] = lbl.transform(list(df[colname].values))\n",
    "        \n",
    "    if one_hot:\n",
    "        return pd.get_dummies(df,columns=cat_cols,dummy_na=True)\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "\n",
    "def get_iqr_min_max(df,cols):\n",
    "    out = {}\n",
    "    for colname, col_values in df.items():\n",
    "        if colname not in cols:\n",
    "            continue\n",
    "        quartile75, quartile25 = np.percentile(col_values, [75 ,25])\n",
    "        ## Inter Quartile Range ##\n",
    "        IQR = quartile75 - quartile25\n",
    "        min_value = quartile25 - (IQR*1.5)\n",
    "        max_value = quartile75 + (IQR*1.5)\n",
    "        out[colname] = (min_value,max_value)\n",
    "    return out\n",
    "\n",
    "def remove_skew(df,threshold=0.75,lambda_in=0.15):\n",
    "    cat_cols, num_cols = get_cat_and_numerical_cols(df)\n",
    "    skewed_cols = df[num_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_cols})\n",
    "    skewness_log = skewness[abs(skewness) > threshold]\n",
    "    skewness_other = skewness[abs(skewness) <= threshold]\n",
    "    skewed_features_log = skewness_log.index\n",
    "    skewed_features_other = skewness_other.index\n",
    "    lambda_ = 0.0\n",
    "    for feature in skewed_features_log:\n",
    "        df[feature] = boxcox1p(df[feature],lambda_)\n",
    "        lambda_ = lambda_in\n",
    "    for feature in skewed_features_other:\n",
    "        df[feature] = boxcox1p(df[feature],lambda_)\n",
    "    return df\n",
    "\n",
    "def bin_numerical_columns(df_in,cols,inplace=False):\n",
    "    if(inplace):\n",
    "        df = df_in\n",
    "    else:\n",
    "        df = df_in.copy()\n",
    "        \n",
    "    for col in cols.keys():\n",
    "        bins = cols[col]\n",
    "        buckets_ = np.linspace(bins[0],bins[1],bins[2])\n",
    "        df[col] = pd.cut(df[col],buckets_,include_lowest=True)\n",
    "    return df\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_df2(df,id_col= None,df_test=None,test_id=None,\n",
    "                   new_features_func=None,\n",
    "                   date_col=None,\n",
    "                   convert_to_cat_cols=None,\n",
    "                   bin_columns_dict=None,\n",
    "                   remove_skewness=False,\n",
    "                   skew_threshold=0.75,\n",
    "                   boxcox_lambda=0.15\n",
    "                  ):\n",
    "    \n",
    "            \n",
    "    if id_col is not None:\n",
    "        combined.drop(id_col, axis=1,inplace=True)\n",
    "        if df_test is not None and test_id is not None:\n",
    "            test_id = df_test[id_col].copy()\n",
    "        else: test_id = None\n",
    "           \n",
    "   \n",
    "    if new_features_func is not None:\n",
    "        df = new_features_func(df)\n",
    "    \n",
    "        \n",
    "    if convert_to_cat_cols is not None:\n",
    "        df = convert_to_str_type(df,convert_to_cat_cols,inplace=True)\n",
    "        \n",
    "    if bin_columns_dict is not None:\n",
    "        df = bin_numerical_columns(df,bin_columns_dict,inplace=True)\n",
    "    \n",
    "    return df,test_id\n",
    "\n",
    "def create_cleaning_pipelines(log_y=False,one_hot=False):\n",
    "    def log_of_y(y):\n",
    "        if log_y:\n",
    "            return np.log1p(y)\n",
    "        else: \n",
    "            return y\n",
    "        \n",
    "    pipeline_y = make_pipeline(FunctionTransformer(log_of_y))\n",
    "    pipeline_df_missing_num = make_pipeline(Imputer(strategy='median',axis=0))\n",
    "    pipeline_df_missing_cat = make_pipeline(Imputer(strategy='most_frequent',axis=0)\n",
    "                                            \n",
    "                                            )\n",
    "    \n",
    "    return {'pipe_y':pipeline_y,\n",
    "            'pipe_df_missing_num':pipeline_df_missing_num,\n",
    "            'pipe_df_missing_cat':pipeline_df_missing_cat\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features1(df):\n",
    "    print('In add new features 1')\n",
    "    print(df.head())\n",
    "    df['DepsIncomeComined'] = df['NumberOfDependents'] * df['MonthlyIncome']\n",
    "    df['Times90DaysLateDebtRatio'] = df['NumberOfTimes90DaysLate'] * df['DebtRatio']\n",
    "    df['Times90DaysLateRevolving'] = df['NumberOfTimes90DaysLate'] * df['RevolvingUtilizationOfUnsecuredLines']\n",
    "    return df\n",
    "def add_new_features2(df):\n",
    "    df['DepsIncomeComined'] = df['NumberOfDependents'] * df['MonthlyIncome']\n",
    "    df['Times90DaysLateDebtRatio'] = df['NumberOfTimes90DaysLate'] * df['DebtRatio']\n",
    "    df['Times90DaysLateRevolving'] = df['NumberOfTimes90DaysLate'] * df['RevolvingUtilizationOfUnsecuredLines']\n",
    "    df['RevolvingUtilizationOfUnsecuredLines-2'] = df['RevolvingUtilizationOfUnsecuredLines'] ** 2\n",
    "    df['RevolvingUtilizationOfUnsecuredLines-3'] = df['RevolvingUtilizationOfUnsecuredLines'] ** 3\n",
    "    df['RevolvingUtilizationOfUnsecuredLines-sqrt'] = np.sqrt(df['RevolvingUtilizationOfUnsecuredLines'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251503, 16)\n",
      "len of cat cols = 2\n",
      "combined shape = (251503, 46)\n",
      "(251503, 46)\n",
      "(150000, 1)\n",
      "0.0\n",
      "   SeriousDlqin2yrs\n",
      "0               1.0\n",
      "1               0.0\n",
      "2               0.0\n",
      "3               0.0\n",
      "4               0.0\n",
      "(150000,)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"data/give_me_credit/\"\n",
    "df_raw = pd.read_csv(f'{PATH}train.csv', low_memory=False)\n",
    "df_test = pd.read_csv(f'{PATH}test.csv', low_memory=False)\n",
    "columns = ['Id', 'SeriousDlqin2yrs','RevolvingUtilizationOfUnsecuredLines', 'age',\n",
    "                 'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',\n",
    "                 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n",
    "                 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "                 'NumberOfDependents']\n",
    "\n",
    "columns = [col.replace('-','') for col in columns]\n",
    "    \n",
    "df_raw.columns= columns\n",
    "df_test.columns = columns\n",
    "df_test.drop(['SeriousDlqin2yrs'], axis=1, inplace=True)\n",
    "df = df_raw.copy()\n",
    "df,y = extract_and_drop_target_column(df,'SeriousDlqin2yrs',inplace=True)\n",
    "\n",
    "n_train = df.shape[0]\n",
    "n_test = df_test.shape[0]\n",
    "    \n",
    "pipelines = create_cleaning_pipelines(log_y=True)\n",
    "combined = pd.concat((df, df_test)).reset_index(drop=True)\n",
    "combined,test_id = preprocess_df2(combined,id_col='Id',\n",
    "                                    df_test=df_test,test_id='Id',\n",
    "                                    new_features_func=add_new_features2,\n",
    "                                    convert_to_cat_cols=[\n",
    "                                            'NumberOfTime3059DaysPastDueNotWorse',\n",
    "                                            'NumberOfTime6089DaysPastDueNotWorse'\n",
    "                                     ]\n",
    "                                \n",
    "                                   )\n",
    "print(combined.shape)\n",
    "combined = handle_encoding(combined,one_hot=True)\n",
    "print('combined shape = {}'.format(combined.shape) )\n",
    "combined = clean_df(combined,pipelines)\n",
    "\n",
    "combined = remove_skew(combined,threshold=0.75,lambda_in=0.15)\n",
    "\n",
    "get_missing_values_percentage(combined)\n",
    "\n",
    "y = clean_df(pd.DataFrame(y),pipelines)\n",
    "\n",
    "print(get_missing_values_percentage(y))\n",
    "print(y.head())\n",
    "y = y.values.ravel()\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([('robust',RobustScaler()),\n",
    "                      ('GBClassifier',GradientBoostingClassifier())])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline([('Robust',RobustScaler()),\n",
    "                           ('Standard',StandardScaler()),\n",
    "                           ('GBClassifier',GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'robust': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "        with_scaling=True),\n",
       " 'GBClassifier': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               n_iter_no_change=None, presort='auto', random_state=None,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('robust',\n",
       "  RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "         with_scaling=True)),\n",
       " ('GBClassifier',\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                n_iter_no_change=None, presort='auto', random_state=None,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'friedman_mse'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1.named_steps['GBClassifier'].criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_line_fit(self, X, y):\n",
    "    X_transformed = X\n",
    "    for name, estimator in self.steps[:-1]:\n",
    "        X_transformed = estimator.fit_transform(X_transformed, y)\n",
    "    self.steps[-1][1].fit(X_transformed, y)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipeline1 = make_pipeline(RobustScaler(),\n",
    "                                     GradientBoostingClassifier())\n",
    "\n",
    "processing_pipeline2 = make_pipeline(RobustScaler(),\n",
    "                                     StandardScaler(),\n",
    "                                     GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipeline3 = make_pipeline(RobustScaler(),\n",
    "                                     StandardScaler(),\n",
    "                                     PolynomialFeatures(),\n",
    "                                     GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108000, 46), (27000, 46), (108000,), (27000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = combined[:n_train]\n",
    "df_test = combined[n_train:]\n",
    "stratify_col = y\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df,y,test_size=0.10,\n",
    "                                  stratify=y,shuffle = True,random_state=20)\n",
    "\n",
    "stratify_X_train = stratify_col[:X_train.shape[0]].copy()\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape, stratify_X_train.shape\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size=0.2,\n",
    "                                  stratify=stratify_X_train,shuffle = True,random_state=20)\n",
    "X_train.shape,X_valid.shape,y_train.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Gradient Boost Pipeline1: 0.859772\n"
     ]
    }
   ],
   "source": [
    "pipe_model1 = processing_pipeline1.fit(X_train,y_train)\n",
    "pipe1_auc = roc_auc_score(y_valid,pipe_model1.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC for Gradient Boost Pipeline1: {:.6f}\".format(pipe1_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Gradient Boost Pipeline2: 0.859774\n"
     ]
    }
   ],
   "source": [
    "pipe_model2 = processing_pipeline2.fit(X_train,y_train)\n",
    "pipe2_auc = roc_auc_score(y_valid,pipe_model2.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC for Gradient Boost Pipeline2: {:.6f}\".format(pipe2_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[24893   246]\n",
      " [ 1505   356]]\n"
     ]
    }
   ],
   "source": [
    "preds_model1 = pipe_model1.predict(X_valid)\n",
    "confusion = confusion_matrix(y_valid, preds_model1)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[24893   246]\n",
      " [ 1505   356]]\n"
     ]
    }
   ],
   "source": [
    "preds_model2 = pipe_model2.predict(X_valid)\n",
    "confusion = confusion_matrix(y_valid, preds_model2)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 43.96106515328089 mins\n",
      "AUC for Randomized Search Gradient Boost: 0.861863\n"
     ]
    }
   ],
   "source": [
    "params = {'gradientboostingclassifier__n_estimators':[200,300,500,800,1000,1500],\n",
    "          \"gradientboostingclassifier__max_features\": randint(10,45),\n",
    "          \"gradientboostingclassifier__min_samples_split\": randint(2, 11),\n",
    "          \"gradientboostingclassifier__min_samples_leaf\": randint(1, 11),\n",
    "          \"gradientboostingclassifier__subsample\":[0.6,0.7,0.75,0.8,0.9]\n",
    "         }\n",
    "\n",
    "kfold = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "start = time()\n",
    "randomSearch_p2 = RandomizedSearchCV(processing_pipeline2,\n",
    "                                     param_distributions=params,\n",
    "                                     n_iter=20,n_jobs=6,\n",
    "                                     scoring='roc_auc',\n",
    "                                     cv=kfold).fit(X_train,y_train)\n",
    "\n",
    "print('training took {} mins'.format((time() - start)/60.))\n",
    "randomSearch_p2_auc = roc_auc_score(y_valid,randomSearch_p2.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC for Randomized Search Gradient Boost: {:.6f}\".format(randomSearch_p2_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.864 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 17, 'gradientboostingclassifier__min_samples_leaf': 7, 'gradientboostingclassifier__min_samples_split': 7, 'gradientboostingclassifier__n_estimators': 300, 'gradientboostingclassifier__subsample': 0.75}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.864 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 24, 'gradientboostingclassifier__min_samples_leaf': 6, 'gradientboostingclassifier__min_samples_split': 5, 'gradientboostingclassifier__n_estimators': 300, 'gradientboostingclassifier__subsample': 0.7}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.864 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 10, 'gradientboostingclassifier__min_samples_leaf': 8, 'gradientboostingclassifier__min_samples_split': 4, 'gradientboostingclassifier__n_estimators': 800, 'gradientboostingclassifier__subsample': 0.75}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(randomSearch_p2,'randomSearch_pipe2_credit.pkl')\n",
    "report_best_scores(randomSearch_p2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.864 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 17, 'gradientboostingclassifier__min_samples_leaf': 7, 'gradientboostingclassifier__min_samples_split': 7, 'gradientboostingclassifier__n_estimators': 300, 'gradientboostingclassifier__subsample': 0.75}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.864 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 24, 'gradientboostingclassifier__min_samples_leaf': 6, 'gradientboostingclassifier__min_samples_split': 5, 'gradientboostingclassifier__n_estimators': 300, 'gradientboostingclassifier__subsample': 0.7}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.864 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 10, 'gradientboostingclassifier__min_samples_leaf': 8, 'gradientboostingclassifier__min_samples_split': 4, 'gradientboostingclassifier__n_estimators': 800, 'gradientboostingclassifier__subsample': 0.75}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomSearch_p2 = joblib.load('randomSearch_pipe2_credit.pkl')\n",
    "report_best_scores(randomSearch_p2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Randomized Search Gradient Boost: 0.861863\n"
     ]
    }
   ],
   "source": [
    "randomSearch_p2 = joblib.load('randomSearch_pipe2_credit.pkl')\n",
    "randomSearch_p2_auc = roc_auc_score(y_valid,randomSearch_p2.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC for Randomized Search Gradient Boost: {:.6f}\".format(randomSearch_p2_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test for P2: 0.867854\n"
     ]
    }
   ],
   "source": [
    "rand_p2_auc = roc_auc_score(y_test,randomSearch_p2.predict_proba(X_test)[:, 1])\n",
    "print(\"AUC on Test for P2: {:.6f}\".format(rand_p2_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[24881   258]\n",
      " [ 1514   347]]\n"
     ]
    }
   ],
   "source": [
    "preds_rand_p2 = randomSearch_p2.predict(X_valid)\n",
    "confusion = confusion_matrix(y_valid, preds_rand_p2)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251503, 10)\n",
      "len of cat cols = 0\n",
      "combined shape = (251503, 10)\n",
      "(251503, 10)\n",
      "(150000, 1)\n",
      "0.0\n",
      "   SeriousDlqin2yrs\n",
      "0               1.0\n",
      "1               0.0\n",
      "2               0.0\n",
      "3               0.0\n",
      "4               0.0\n",
      "(150000,)\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(f'{PATH}train.csv', low_memory=False)\n",
    "df_test = pd.read_csv(f'{PATH}test.csv', low_memory=False)\n",
    "columns = ['Id', 'SeriousDlqin2yrs','RevolvingUtilizationOfUnsecuredLines', 'age',\n",
    "                 'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',\n",
    "                 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n",
    "                 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "                 'NumberOfDependents']\n",
    "\n",
    "columns = [col.replace('-','') for col in columns]\n",
    "    \n",
    "df_raw.columns= columns\n",
    "df_test.columns = columns\n",
    "df_test.drop(['SeriousDlqin2yrs'], axis=1, inplace=True)\n",
    "df = df_raw.copy()\n",
    "df,y = extract_and_drop_target_column(df,'SeriousDlqin2yrs',inplace=True)\n",
    "\n",
    "n_train = df.shape[0]\n",
    "n_test = df_test.shape[0]\n",
    "    \n",
    "pipelines = create_cleaning_pipelines(log_y=True)\n",
    "combined = pd.concat((df, df_test)).reset_index(drop=True)\n",
    "combined,test_id = preprocess_df2(combined,id_col='Id',\n",
    "                                    df_test=df_test,\n",
    "                                    test_id='Id'\n",
    "                                \n",
    "                                   )\n",
    "print(combined.shape)\n",
    "combined = handle_encoding(combined,one_hot=True)\n",
    "print('combined shape = {}'.format(combined.shape) )\n",
    "combined = clean_df(combined,pipelines)\n",
    "\n",
    "combined = remove_skew(combined,threshold=0.75,lambda_in=0.2)\n",
    "\n",
    "get_missing_values_percentage(combined)\n",
    "\n",
    "y = clean_df(pd.DataFrame(y),pipelines)\n",
    "\n",
    "print(get_missing_values_percentage(y))\n",
    "print(y.head())\n",
    "y = y.values.ravel()\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108000, 10), (27000, 10), (108000,), (27000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = combined[:n_train]\n",
    "df_test = combined[n_train:]\n",
    "stratify_col = y\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df,y,test_size=0.10,\n",
    "                                  stratify=y,shuffle = True,random_state=20)\n",
    "\n",
    "stratify_X_train = stratify_col[:X_train.shape[0]].copy()\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape, stratify_X_train.shape\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size=0.2,\n",
    "                                  stratify=stratify_X_train,shuffle = True,random_state=20)\n",
    "X_train.shape,X_valid.shape,y_train.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Gradient Boost Pipeline3: 0.859008\n"
     ]
    }
   ],
   "source": [
    "pipe_model3 = processing_pipeline3.fit(X_train,y_train)\n",
    "pipe3_auc = roc_auc_score(y_valid,pipe_model3.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC for Gradient Boost Pipeline3: {:.6f}\".format(pipe3_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 35.36313624779383 mins\n",
      "AUC on Randomized Search for P3: 0.861847\n"
     ]
    }
   ],
   "source": [
    "params = {'gradientboostingclassifier__n_estimators':[200,300,500,800,1000,1500],\n",
    "          'gradientboostingclassifier__max_features': randint(1,11),\n",
    "          'gradientboostingclassifier__min_samples_split': randint(2, 11),\n",
    "          'gradientboostingclassifier__min_samples_leaf': randint(1, 11),\n",
    "          'gradientboostingclassifier__subsample':[0.6,0.7,0.8,0.9],\n",
    "          'polynomialfeatures__degree': [1,2,3],\n",
    "          \n",
    "         }\n",
    "\n",
    "kfold = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "start = time()\n",
    "randomSearch_p3 = RandomizedSearchCV(processing_pipeline3,\n",
    "                                     param_distributions=params,\n",
    "                                     n_iter=20,n_jobs=6,\n",
    "                                     scoring='roc_auc',\n",
    "                                     cv=kfold).fit(X_train,y_train)\n",
    "\n",
    "print('training took {} mins'.format((time() - start)/60.))\n",
    "randomSearch_p3_auc = roc_auc_score(y_valid,randomSearch_p3.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC on Randomized Search for P3: {:.6f}\".format(randomSearch_p3_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.865 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 1, 'gradientboostingclassifier__min_samples_leaf': 2, 'gradientboostingclassifier__min_samples_split': 8, 'gradientboostingclassifier__n_estimators': 500, 'gradientboostingclassifier__subsample': 0.9, 'polynomialfeatures__degree': 1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.865 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 2, 'gradientboostingclassifier__min_samples_leaf': 9, 'gradientboostingclassifier__min_samples_split': 4, 'gradientboostingclassifier__n_estimators': 500, 'gradientboostingclassifier__subsample': 0.6, 'polynomialfeatures__degree': 1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.865 (std: 0.005)\n",
      "Parameters: {'gradientboostingclassifier__max_features': 10, 'gradientboostingclassifier__min_samples_leaf': 4, 'gradientboostingclassifier__min_samples_split': 2, 'gradientboostingclassifier__n_estimators': 300, 'gradientboostingclassifier__subsample': 0.8, 'polynomialfeatures__degree': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(randomSearch_p3,'./randomSearch_pipe3_credit.pkl')\n",
    "report_best_scores(randomSearch_p3.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Randomized Search Gradient Boost: 0.861847\n"
     ]
    }
   ],
   "source": [
    "randomSearch_p3 = joblib.load('./randomSearch_pipe3_credit.pkl')\n",
    "randomSearch_p3_auc = roc_auc_score(y_valid,randomSearch_p3.predict_proba(X_valid)[:, 1])\n",
    "print(\"AUC for Randomized Search Gradient Boost: {:.6f}\".format(randomSearch_p3_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Test for P3: 0.867939\n"
     ]
    }
   ],
   "source": [
    "rand_p3_auc = roc_auc_score(y_test,randomSearch_p3.predict_proba(X_test)[:, 1])\n",
    "print(\"AUC on Test for P3: {:.6f}\".format(rand_p3_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
