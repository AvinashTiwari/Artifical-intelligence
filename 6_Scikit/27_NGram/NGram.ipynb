{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\"The quick brown fox jumped over the lazy dog\",\n",
    "           \"education is what you have left over after forgetting everything you ever learnt\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 19\n",
      "Vocabulary:\n",
      "['after', 'brown', 'dog', 'education', 'ever', 'everything', 'forgetting', 'fox', 'have', 'is', 'jumped', 'lazy', 'learnt', 'left', 'over', 'quick', 'the', 'what', 'you']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 1)).fit(phrases)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18\n",
      "Vocabulary:\n",
      "['after forgetting everything', 'brown fox jumped', 'education is what', 'everything you ever', 'forgetting everything you', 'fox jumped over', 'have left over', 'is what you', 'jumped over the', 'left over after', 'over after forgetting', 'over the lazy', 'quick brown fox', 'the lazy dog', 'the quick brown', 'what you have', 'you ever learnt', 'you have left']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(3, 3)).fit(phrases)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 57\n",
      "Vocabulary:\n",
      "['after', 'after forgetting', 'after forgetting everything', 'brown', 'brown fox', 'brown fox jumped', 'dog', 'education', 'education is', 'education is what', 'ever', 'ever learnt', 'everything', 'everything you', 'everything you ever', 'forgetting', 'forgetting everything', 'forgetting everything you', 'fox', 'fox jumped', 'fox jumped over', 'have', 'have left', 'have left over', 'is', 'is what', 'is what you', 'jumped', 'jumped over', 'jumped over the', 'lazy', 'lazy dog', 'learnt', 'left', 'left over', 'left over after', 'over', 'over after', 'over after forgetting', 'over the', 'over the lazy', 'quick', 'quick brown', 'quick brown fox', 'the', 'the lazy', 'the lazy dog', 'the quick', 'the quick brown', 'what', 'what you', 'what you have', 'you', 'you ever', 'you ever learnt', 'you have', 'you have left']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 3)).fit(phrases)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_split(data,y,length,split_mark=0.7):\n",
    "    if split_mark > 0. and split_mark < 1.0:\n",
    "        n = int(split_mark*length)\n",
    "    else:\n",
    "        n = int(split_mark)\n",
    "    X_train =  data[:n].copy()\n",
    "    X_test =   data[n:].copy()\n",
    "    y_train = y[:n].copy()\n",
    "    y_test  = y[n:].copy()\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 3) (7500, 3) (17500,) (7500,)\n"
     ]
    }
   ],
   "source": [
    "d_train,d_test,y_train,y_test = simple_split(data,data.sentiment,len(data))\n",
    "print(d_train.shape,d_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.90\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
    "LogisticRegression())\n",
    "\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(d_train.review, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'logisticregression__C': 0.001, 'tfidfvectorizer__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.900\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None,ngram_range=(1,3)),\n",
    "LogisticRegression(C=0.001))\n",
    "pipe.fit(d_train.review, y_train)\n",
    "print(\"Test set score: {:.3f}\".format(pipe.score(d_test.review, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[8399  394]\n",
      " [ 372 8335]]\n"
     ]
    }
   ],
   "source": [
    "pred_logreg = pipe.predict(d_test.review)\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.973\n",
      "Test set score: 0.890\n"
     ]
    }
   ],
   "source": [
    "pipe2 = make_pipeline(TfidfVectorizer(min_df=5, norm=None,ngram_range=(1,3)),\n",
    "MultinomialNB())\n",
    "pipe2.fit(d_train.review, y_train)\n",
    "print(\"Train set score: {:.3f}\".format(pipe2.score(d_train.review, y_train)))     \n",
    "print(\"Test set score: {:.3f}\".format(pipe2.score(d_test.review, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[3338  401]\n",
      " [ 425 3336]]\n"
     ]
    }
   ],
   "source": [
    "pred_nb = pipe2.predict(d_test.review)\n",
    "confusion = confusion_matrix(y_test, pred_nb)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 1.000\n",
      "Test set score: 0.871\n"
     ]
    }
   ],
   "source": [
    "pipe3 = make_pipeline(TfidfVectorizer(min_df=5, norm=None,ngram_range=(1,3)),\n",
    "                     RandomForestClassifier(n_estimators=1000,n_jobs=4))\n",
    "pipe3.fit(d_train.review, y_train)\n",
    "print(\"Train set score: {:.3f}\".format(pipe3.score(d_train.review, y_train)))     \n",
    "print(\"Test set score: {:.3f}\".format(pipe3.score(d_test.review, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[3204  535]\n",
      " [ 435 3326]]\n"
     ]
    }
   ],
   "source": [
    "pred_rf = pipe3.predict(d_test.review)\n",
    "confusion = confusion_matrix(y_test, pred_rf)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.924\n",
      "Test set score: 0.870\n"
     ]
    }
   ],
   "source": [
    "pipe4 = make_pipeline(TfidfVectorizer(min_df=5, norm=None,ngram_range=(1,3)),\n",
    "                     GradientBoostingClassifier(n_estimators=500))\n",
    "pipe4.fit(d_train.review, y_train)\n",
    "print(\"Train set score: {:.3f}\".format(pipe4.score(d_train.review, y_train)))     \n",
    "print(\"Test set score: {:.3f}\".format(pipe4.score(d_test.review, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[3179  560]\n",
      " [ 413 3348]]\n"
     ]
    }
   ],
   "source": [
    "pred_gb = pipe4.predict(d_test.review)\n",
    "confusion = confusion_matrix(y_test, pred_gb)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "review = [\"This movie is not that good\"]\n",
    "print(pipe.predict(review)[0])\n",
    "print(pipe2.predict(review)[0])\n",
    "print(pipe3.predict(review)[0])\n",
    "print(pipe4.predict(review)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "review = [\"This movie is not that bad\"]\n",
    "print(pipe.predict(review)[0])\n",
    "print(pipe2.predict(review)[0])\n",
    "print(pipe3.predict(review)[0])\n",
    "print(pipe4.predict(review)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "review = [\"I was going to say something awesome or great or good, but I can't because the movie is so bad.\"]\n",
    "print(pipe.predict(review)[0])\n",
    "print(pipe2.predict(review)[0])\n",
    "print(pipe3.predict(review)[0])\n",
    "print(pipe4.predict(review)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
