# -*- coding: utf-8 -*-
"""Distributed Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RABY1ldyCwtHgXIYx8xzyLd0kI3IfKRk

## Stage 1: Installing dependencies and setting up the environment
"""

!pip install tensorflow-gpu==2.0.0.alpha0

"""## Stage 2: Importing project dependencies"""

import time
import numpy as np
import tensorflow as tf

tf.__version__

"""## Stage 3: Dataset preprocessing

### Loading the MNIST dataset
"""

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

"""### Image normalization"""

X_train = X_train / 255.
X_test = X_test / 255.

X_train.shape

"""### Dataset reshaping"""

X_train = X_train.reshape(-1, 28*28)
X_test = X_test.reshape(-1, 28*28)

X_train.shape

"""## Stage 4: Distributed Training

### Defining normal (non distributed) model
"""

model_normal = tf.keras.models.Sequential()

model_normal.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)))

model_normal.add(tf.keras.layers.Dropout(rate=0.2))

model_normal.add(tf.keras.layers.Dense(units=10, activation='softmax'))

model_normal.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])

"""### Defining the Distributed Strategy"""

distribute = tf.distribute.MirroredStrategy()

"""### Defining a distributed model"""

with distribute.scope():
  model_distributed = tf.keras.models.Sequential()
  model_distributed.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)))
  model_distributed.add(tf.keras.layers.Dropout(rate=0.2))
  model_distributed.add(tf.keras.layers.Dense(units=10, activation='softmax'))
  model_distributed.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])

"""### Speed comparison between normal training and distributed training process"""

start_time = time.time()
model_distributed.fit(X_train, y_train, epochs=10, batch_size=25)
print("Distributed training took: {}".format(time.time() - start_time))

start_time = time.time()
model_normal.fit(X_train, y_train, epochs=10, batch_size=25)
print("Normal training took: {}".format(time.time() - start_time))

